/*
 * Copyright 1993-2010 NVIDIA Corporation.  All rights reserved.
 *
 * Please refer to the NVIDIA end user license agreement (EULA) associated
 * with this source code for terms and conditions that govern your use of
 * this software. Any use, reproduction, disclosure, or distribution of
 * this software and related documentation outside the terms of the EULA
 * is strictly prohibited.
 *
 */
 
 /*
 * Copyright 1993-2010 NVIDIA Corporation.  All rights reserved.
 * 
 * Tridiagonal solvers.
 * Device code for cyclic reduction (CR).
 *
 * Original CUDA kernel: UC Davis, Yao Zhang & John Owens, 2009
 * 
 * NVIDIA, Nikolai Sakharnykh, 2009
 */

#define NATIVE_DIVIDE

__kernel void cyclic_small_systems_kernel(__global float *a_d, __global float *b_d, __global float *c_d, __global float *d_d, __global float *x_d,
										  __local float *shared, int system_size, int num_systems, int iterations)
{
    int thid = get_local_id(0);
    int blid = get_group_id(0);

	int stride = 1;
    int half_size = system_size >> 1;
	int thid_num = half_size;

	__local float* a = shared;
	__local float* b = &a[system_size];
	__local float* c = &b[system_size];
	__local float* d = &c[system_size];
	__local float* x = &d[system_size];

	a[thid] = a_d[thid + blid * system_size];
	a[thid + thid_num] = a_d[thid + thid_num + blid * system_size];

	b[thid] = b_d[thid + blid * system_size];
	b[thid + thid_num] = b_d[thid + thid_num + blid * system_size];

	c[thid] = c_d[thid + blid * system_size];
	c[thid + thid_num] = c_d[thid + thid_num + blid * system_size];

	d[thid] = d_d[thid + blid * system_size];
	d[thid + thid_num] = d_d[thid + thid_num + blid * system_size];
	
	barrier(CLK_LOCAL_MEM_FENCE);

	// forward elimination
	for (int j = 0; j < iterations; j++)
	{
		barrier(CLK_LOCAL_MEM_FENCE);

		stride <<= 1;
		int delta = stride >> 1;
        if (thid < thid_num)
		{ 
			int i = stride * thid + stride - 1;

			if (i == system_size - 1)
			{
#ifndef NATIVE_DIVIDE
				float tmp = a[i] / b[i-delta];
#else
				float tmp = native_divide(a[i], b[i-delta]);
#endif
				b[i] = b[i] - c[i-delta] * tmp;
				d[i] = d[i] - d[i-delta] * tmp;
				a[i] = -a[i-delta] * tmp;
				c[i] = 0;			
			}
			else
			{
#ifndef NATIVE_DIVIDE
				float tmp1 = a[i] / b[i-delta];
				float tmp2 = c[i] / b[i+delta];
#else
				float tmp1 = native_divide(a[i], b[i-delta]);
				float tmp2 = native_divide(c[i], b[i+delta]);
#endif
				b[i] = b[i] - c[i-delta] * tmp1 - a[i+delta] * tmp2;
				d[i] = d[i] - d[i-delta] * tmp1 - d[i+delta] * tmp2;
				a[i] = -a[i-delta] * tmp1;
				c[i] = -c[i+delta] * tmp2;
			}
		}
        thid_num >>= 1;
	}

    if (thid < 2)
    {
		int addr1 = stride - 1;
		int addr2 = (stride << 1) - 1;
		float tmp3 = b[addr2] * b[addr1] - c[addr1] * a[addr2];
#ifndef NATIVE_DIVIDE
		x[addr1] = (b[addr2] * d[addr1] - c[addr1] * d[addr2]) / tmp3;
		x[addr2] = (d[addr2] * b[addr1] - d[addr1] * a[addr2]) / tmp3;
#else
		x[addr1] = native_divide((b[addr2] * d[addr1] - c[addr1] * d[addr2]), tmp3);
		x[addr2] = native_divide((d[addr2] * b[addr1] - d[addr1] * a[addr2]), tmp3);
#endif
    }

    // backward substitution
    thid_num = 2;
    for (int j = 0; j < iterations; j++)
	{
		int delta = stride >> 1;
		barrier(CLK_LOCAL_MEM_FENCE);
		if (thid < thid_num)
        {
            int i = stride * thid + (stride >> 1) - 1;
#ifndef NATIVE_DIVIDE
            if (i == delta - 1)
                x[i] = (d[i] - c[i] * x[i+delta]) / b[i];
		    else
		        x[i] = (d[i] - a[i] * x[i-delta] - c[i] * x[i+delta]) / b[i];
#else
            if (i == delta - 1)
                x[i] = native_divide((d[i] - c[i] * x[i+delta]), b[i]);
		    else
		        x[i] = native_divide((d[i] - a[i] * x[i-delta] - c[i] * x[i+delta]), b[i]);
#endif
         }
		 stride >>= 1;
         thid_num <<= 1;
	}

	barrier(CLK_LOCAL_MEM_FENCE);   

    x_d[thid + blid * system_size] = x[thid];
	x_d[thid + half_size + blid * system_size] = x[thid + half_size];
}

__kernel void cyclic_branch_free_kernel(__global float *a_d, __global float *b_d, __global float *c_d, __global float *d_d, __global float *x_d,
										__local float *shared, int system_size, int num_systems, int iterations)
{
    int thid = get_local_id(0);
    int blid = get_group_id(0);

	int stride = 1;
    int half_size = system_size >> 1;
	int thid_num = half_size;

	__local float* a = shared;
	__local float* b = &a[system_size];
	__local float* c = &b[system_size];
	__local float* d = &c[system_size];
	__local float* x = &d[system_size];

	a[thid] = a_d[thid + blid * system_size];
	a[thid + thid_num] = a_d[thid + thid_num + blid * system_size];

	b[thid] = b_d[thid + blid * system_size];
	b[thid + thid_num] = b_d[thid + thid_num + blid * system_size];

	c[thid] = c_d[thid + blid * system_size];
	c[thid + thid_num] = c_d[thid + thid_num + blid * system_size];

	d[thid] = d_d[thid + blid * system_size];
	d[thid + thid_num] = d_d[thid + thid_num + blid * system_size];
	
	barrier(CLK_LOCAL_MEM_FENCE);

	// forward elimination
	for (int j = 0; j < iterations; j++)
	{
		barrier(CLK_LOCAL_MEM_FENCE);

		stride <<= 1;
		int delta = stride >> 1;
        if (thid < thid_num)
		{ 
			int i = stride * thid + stride - 1;
			int iRight = i+delta;
			iRight = iRight & (system_size-1);
#ifndef NATIVE_DIVIDE
			float tmp1 = a[i] / b[i-delta];
			float tmp2 = c[i] / b[iRight];
#else
			float tmp1 = native_divide(a[i], b[i-delta]);
			float tmp2 = native_divide(c[i], b[iRight]);
#endif
			b[i] = b[i] - c[i-delta] * tmp1 - a[iRight] * tmp2;
			d[i] = d[i] - d[i-delta] * tmp1 - d[iRight] * tmp2;
			a[i] = -a[i-delta] * tmp1;
			c[i] = -c[iRight]  * tmp2;
		}

        thid_num >>= 1;
	}

    if (thid < 2)
    {
		int addr1 = stride - 1;
		int addr2 = (stride << 1) - 1;
		float tmp3 = b[addr2] * b[addr1] - c[addr1] * a[addr2];
#ifndef NATIVE_DIVIDE
		x[addr1] = (b[addr2] * d[addr1] - c[addr1] * d[addr2]) / tmp3;
		x[addr2] = (d[addr2] * b[addr1] - d[addr1] * a[addr2]) / tmp3;
#else
		x[addr1] = native_divide((b[addr2] * d[addr1] - c[addr1] * d[addr2]), tmp3);
		x[addr2] = native_divide((d[addr2] * b[addr1] - d[addr1] * a[addr2]), tmp3);
#endif
    }

    // backward substitution
    thid_num = 2;
    for (int j = 0; j < iterations; j++)
	{
		int delta = stride >> 1;
		barrier(CLK_LOCAL_MEM_FENCE);
		if (thid < thid_num)
        {
            int i = stride * thid + (stride >> 1) - 1;
#ifndef NATIVE_DIVIDE
            if (i == delta - 1)
                x[i] = (d[i] - c[i] * x[i+delta]) / b[i];
		    else
		        x[i] = (d[i] - a[i] * x[i-delta] - c[i] * x[i+delta]) / b[i];
#else
            if (i == delta - 1)
                x[i] = native_divide((d[i] - c[i] * x[i+delta]), b[i]);
		    else
		        x[i] = native_divide((d[i] - a[i] * x[i-delta] - c[i] * x[i+delta]), b[i]);
#endif
         }
		 stride >>= 1;
         thid_num <<= 1;
	}

	barrier(CLK_LOCAL_MEM_FENCE);   

    x_d[thid + blid * system_size] = x[thid];
	x_d[thid + half_size + blid * system_size] = x[thid + half_size];
}

/*
 * Copyright 1993-2010 NVIDIA Corporation.  All rights reserved.
 *
 * Please refer to the NVIDIA end user license agreement (EULA) associated
 * with this source code for terms and conditions that govern your use of
 * this software. Any use, reproduction, disclosure, or distribution of
 * this software and related documentation outside the terms of the EULA
 * is strictly prohibited.
 *
 */
 
 /*
 * Copyright 1993-2010 NVIDIA Corporation.  All rights reserved.
 * 
 * Tridiagonal solvers.
 * Device code for parallel cyclic reduction (PCR).
 *
 * Original CUDA kernels: UC Davis, Yao Zhang & John Owens, 2009
 * 
 * NVIDIA, Nikolai Sakharnykh, 2009
 */

#define NATIVE_DIVIDE

__kernel void pcr_small_systems_kernel(__global float *a_d, __global float *b_d, __global float *c_d, __global float *d_d, __global float *x_d, 
									   __local float *shared, int system_size, int num_systems, int iterations)
{
    int thid = get_local_id(0);
    int blid = get_group_id(0);

	int delta = 1;

	__local float* a = shared;
	__local float* b = &a[system_size+1];
	__local float* c = &b[system_size+1];
	__local float* d = &c[system_size+1];
	__local float* x = &d[system_size+1];

	a[thid] = a_d[thid + blid * system_size];
	b[thid] = b_d[thid + blid * system_size];
	c[thid] = c_d[thid + blid * system_size];
	d[thid] = d_d[thid + blid * system_size];
  
	float aNew, bNew, cNew, dNew;
  
	barrier(CLK_LOCAL_MEM_FENCE);

	// parallel cyclic reduction
	for (int j = 0; j < iterations; j++)
	{
		int i = thid;

		if(i < delta)
		{
#ifndef NATIVE_DIVIDE
			float tmp2 = c[i] / b[i+delta];
#else
			float tmp2 = native_divide(c[i], b[i+delta]);
#endif
			bNew = b[i] - a[i+delta] * tmp2;
 			dNew = d[i] - d[i+delta] * tmp2;
			aNew = 0;
			cNew = -c[i+delta] * tmp2;	
		}
		else if((system_size-i-1) < delta)
		{
#ifndef NATIVE_DIVIDE
			float tmp = a[i] / b[i-delta];
#else
			float tmp = native_divide(a[i], b[i-delta]);
#endif
			bNew = b[i] - c[i-delta] * tmp;
			dNew = d[i] - d[i-delta] * tmp;
			aNew = -a[i-delta] * tmp;
			cNew = 0;			
		}
		else		    
		{
#ifndef NATIVE_DIVIDE
			float tmp1 = a[i] / b[i-delta];
			float tmp2 = c[i] / b[i+delta];
#else
			float tmp1 = native_divide(a[i], b[i-delta]);
			float tmp2 = native_divide(c[i], b[i+delta]);
#endif
   			bNew = b[i] - c[i-delta] * tmp1 - a[i+delta] * tmp2;
 			dNew = d[i] - d[i-delta] * tmp1 - d[i+delta] * tmp2;
			aNew = -a[i-delta] * tmp1;
			cNew = -c[i+delta] * tmp2;
		}

		barrier(CLK_LOCAL_MEM_FENCE);
        
		b[i] = bNew;
 		d[i] = dNew;
		a[i] = aNew;
		c[i] = cNew;	
    
		delta *= 2;
		barrier(CLK_LOCAL_MEM_FENCE);
	}

	if (thid < delta)
	{
		int addr1 = thid;
		int addr2 = thid + delta;
		float tmp3 = b[addr2] * b[addr1] - c[addr1] * a[addr2];
#ifndef NATIVE_DIVIDE
		x[addr1] = (b[addr2] * d[addr1] - c[addr1] * d[addr2]) / tmp3;
		x[addr2] = (d[addr2] * b[addr1] - d[addr1] * a[addr2]) / tmp3;
#else
		x[addr1] = native_divide((b[addr2] * d[addr1] - c[addr1] * d[addr2]), tmp3);
		x[addr2] = native_divide((d[addr2] * b[addr1] - d[addr1] * a[addr2]), tmp3);
#endif
	}
    
	barrier(CLK_LOCAL_MEM_FENCE);
    
    x_d[thid + blid * system_size] = x[thid];
}

__kernel void pcr_branch_free_kernel(__global float *a_d, __global float *b_d, __global float *c_d, __global float *d_d, __global float *x_d, 
									 __local float *shared, int system_size, int num_systems, int iterations)
{
	int thid = get_local_id(0);
    int blid = get_group_id(0);

	int delta = 1;

	__local float* a = shared;
	__local float* b = &a[system_size+1];
	__local float* c = &b[system_size+1];
	__local float* d = &c[system_size+1];
	__local float* x = &d[system_size+1];

	a[thid] = a_d[thid + blid * system_size];
	b[thid] = b_d[thid + blid * system_size];
	c[thid] = c_d[thid + blid * system_size];
	d[thid] = d_d[thid + blid * system_size];
  
	float aNew, bNew, cNew, dNew;
  
	barrier(CLK_LOCAL_MEM_FENCE);

	// parallel cyclic reduction
	for (int j = 0; j < iterations; j++)
	{
		int i = thid;

		int iRight = i+delta;
		iRight = iRight & (system_size-1);

		int iLeft = i-delta;
		iLeft = iLeft & (system_size-1);

#ifndef NATIVE_DIVIDE
		float tmp1 = a[i] / b[iLeft];
		float tmp2 = c[i] / b[iRight];
#else
		float tmp1 = native_divide(a[i], b[iLeft]);
		float tmp2 = native_divide(c[i], b[iRight]);
#endif

		bNew = b[i] - c[iLeft] * tmp1 - a[iRight] * tmp2;
		dNew = d[i] - d[iLeft] * tmp1 - d[iRight] * tmp2;
		aNew = -a[iLeft] * tmp1;
		cNew = -c[iRight] * tmp2;

		barrier(CLK_LOCAL_MEM_FENCE);
        
		b[i] = bNew;
 		d[i] = dNew;
		a[i] = aNew;
		c[i] = cNew;	
    
	    delta *= 2;
		barrier(CLK_LOCAL_MEM_FENCE);
	}

	if (thid < delta)
	{
		int addr1 = thid;
		int addr2 = thid + delta;
		float tmp3 = b[addr2] * b[addr1] - c[addr1] * a[addr2];
#ifndef NATIVE_DIVIDE
		x[addr1] = (b[addr2] * d[addr1] - c[addr1] * d[addr2]) / tmp3;
		x[addr2] = (d[addr2] * b[addr1] - d[addr1] * a[addr2]) / tmp3;
#else
		x[addr1] = native_divide((b[addr2] * d[addr1] - c[addr1] * d[addr2]), tmp3);
		x[addr2] = native_divide((d[addr2] * b[addr1] - d[addr1] * a[addr2]), tmp3);
#endif
	}
    
	barrier(CLK_LOCAL_MEM_FENCE);
    
    x_d[thid + blid * system_size] = x[thid];
}/*
 * Copyright 1993-2010 NVIDIA Corporation.  All rights reserved.
 *
 * Please refer to the NVIDIA end user license agreement (EULA) associated
 * with this source code for terms and conditions that govern your use of
 * this software. Any use, reproduction, disclosure, or distribution of
 * this software and related documentation outside the terms of the EULA
 * is strictly prohibited.
 *
 */
 
 /*
 * Copyright 1993-2010 NVIDIA Corporation.  All rights reserved.
 * 
 * Tridiagonal solvers.
 * Device code for sweep solver (one-system-per-thread).
 * 
 * NVIDIA, Nikolai Sakharnykh, 2009
 */

#define NATIVE_DIVIDE

// system_size is defined during program building

// solves a bunch of tridiagonal linear systems
// much better performance when doing data reordering before
// so that all memory accesses are coalesced (who-ho!)
__kernel void sweep_small_systems_local_kernel(__global float *a_d, __global float *b_d, __global float *c_d, __global float *d_d, __global float *x_d, int num_systems)
{
	int i = get_global_id(0);
	
	// need to check for in-bounds because of the thread block size
    if (i >= num_systems) return;

#ifndef REORDER
	int stride = 1;
	int base_idx = i * system_size;
#else
	int stride = num_systems;
	int base_idx = i;
#endif

	// local memory
	float a[system_size];

	float c1, c2, c3;
	float f_i, x_prev, x_next;
	
	// solving next system:	
	// c1 * u_i+1 + c2 * u_i + c3 * u_i-1 = f_i
	
	c1 = c_d[base_idx];
	c2 = b_d[base_idx];
	f_i = d_d[base_idx];

#ifndef NATIVE_DIVIDE
	a[1] = - c1 / c2;
	x_prev = f_i / c2;
#else
	a[1] = - native_divide(c1, c2);
	x_prev = native_divide(f_i, c2);
#endif

	// forward trace
	int idx = base_idx;
	x_d[base_idx] = x_prev;
	for (int k = 1; k < system_size-1; k++)
	{
		idx += stride;
	
		c1 = c_d[idx];
		c2 = b_d[idx];
		c3 = a_d[idx];
		f_i = d_d[idx];
		
		float q = (c3 * a[k] + c2);
#ifndef NATIVE_DIVIDE
		float t = 1 / q; 
#else
		float t = native_recip(q);
#endif
		x_next = (f_i - c3 * x_prev) * t;
		x_d[idx] = x_prev = x_next;
		
		a[k+1] = - c1 * t;
	}
	
	idx += stride;

	c2 = b_d[idx];
	c3 = a_d[idx];
	f_i = d_d[idx];

	float q = (c3 * a[system_size-1] + c2);
#ifndef NATIVE_DIVIDE
	float t = 1 / q; 
#else
	float t = native_recip(q);
#endif 
	x_next = (f_i - c3 * x_prev) * t;
	x_d[idx] = x_prev = x_next;

	// backward trace
	for (int k = system_size-2; k >= 0; k--)
	{
		idx -= stride;
		x_next = x_d[idx];
		x_next += x_prev * a[k+1];
		x_d[idx] = x_prev = x_next;
	}
}

__inline int getLocalIdx(int i, int k, int num_systems)
{
	return i + num_systems * k;

	// uncomment for uncoalesced mem access
	// return k + system_size * i;
}

__kernel void sweep_small_systems_global_kernel(__global float *a_d, __global float *b_d, __global float *c_d, __global float *d_d, __global float *x_d, int num_systems, __global float *w_d)
{
	int i = get_global_id(0);
	
	// need to check for in-bounds because of the thread block size
    if (i >= num_systems) return;

#ifndef REORDER
	int stride = 1;
	int base_idx = i * system_size;
#else
	int stride = num_systems;
	int base_idx = i;
#endif

	float c1, c2, c3;
	float f_i, x_prev, x_next;
	
	// solving next system:	
	// c1 * u_i+1 + c2 * u_i + c3 * u_i-1 = f_i
	
	c1 = c_d[base_idx];
	c2 = b_d[base_idx];
	f_i = d_d[base_idx];

#ifndef NATIVE_DIVIDE
	w_d[getLocalIdx(i, 1, num_systems)] = - c1 / c2;
	x_prev = f_i / c2;
#else
	w_d[getLocalIdx(i, 1, num_systems)] = - native_divide(c1, c2);
	x_prev = native_divide(f_i, c2);
#endif

	// forward trace
	int idx = base_idx;
	x_d[base_idx] = x_prev;
	for (int k = 1; k < system_size-1; k++)
	{
		idx += stride;
	
		c1 = c_d[idx];
		c2 = b_d[idx];
		c3 = a_d[idx];
		f_i = d_d[idx];
		
		float q = (c3 * w_d[getLocalIdx(i, k, num_systems)] + c2);
#ifndef NATIVE_DIVIDE
		float t = 1 / q; 
#else
		float t = native_recip(q);
#endif
		x_next = (f_i - c3 * x_prev) * t;
		x_d[idx] = x_prev = x_next;
		
		w_d[getLocalIdx(i, k+1, num_systems)] = - c1 * t;
	}
	
	idx += stride;

	c2 = b_d[idx];
	c3 = a_d[idx];
	f_i = d_d[idx];

	float q = (c3 * w_d[getLocalIdx(i, system_size-1, num_systems)] + c2);
#ifndef NATIVE_DIVIDE
	float t = 1 / q; 
#else
	float t = native_recip(q);
#endif 
	x_next = (f_i - c3 * x_prev) * t;
	x_d[idx] = x_prev = x_next;

	// backward trace
	for (int k = system_size-2; k >= 0; k--)
	{
		idx -= stride;
		x_next = x_d[idx];
		x_next += x_prev * w_d[getLocalIdx(i, k+1, num_systems)];
		x_d[idx] = x_prev = x_next;
	}
}

__inline float4 load(__global float *a, int i)
{
	return (float4)(a[i], a[i+1], a[i+2], a[i+3]);
}

__inline void store(__global float *a, int i, float4 v)
{
	a[i] = v.x;
	a[i+1] = v.y;
	a[i+2] = v.z;
	a[i+3] = v.w;
}

__kernel void sweep_small_systems_global_vec4_kernel(__global float *a_d, __global float *b_d, __global float *c_d, __global float *d_d, __global float *x_d, int num_systems, __global float *w_d)
{
	int j = get_global_id(0);
	int i = j << 2;
	
	// need to check for in-bounds because of the thread block size
    if (i >= num_systems) return;

#ifndef REORDER
	int stride = 4;
	int base_idx = i * system_size;
#else
	int stride = num_systems;
	int base_idx = i;
#endif

	float4 c1, c2, c3;
	float4 f_i, x_prev, x_next;
	
	// solving next system:	
	// c1 * u_i+1 + c2 * u_i + c3 * u_i-1 = f_i
	
	c1 = load(c_d, base_idx);
	c2 = load(b_d, base_idx);
	f_i = load(d_d, base_idx);

#ifndef NATIVE_DIVIDE
	store(w_d, getLocalIdx(i, 1, num_systems), - c1 / c2);
	x_prev = f_i / c2;
#else
	store(w_d, getLocalIdx(i, 1, num_systems), - native_divide(c1, c2));
	x_prev = native_divide(f_i, c2);
#endif

	// forward trace
	int idx = base_idx;
	store(x_d, base_idx, x_prev);
	for (int k = 1; k < system_size-1; k++)
	{
		idx += stride;
	
		c1 = load(c_d, idx);
		c2 = load(b_d, idx);
		c3 = load(a_d, idx);
		f_i = load(d_d, idx);
		
		float4 q = (c3 * load(w_d, getLocalIdx(i, k, num_systems)) + c2);
#ifndef NATIVE_DIVIDE
		float4 t = float4(1,1,1,1) / q; 
#else
		float4 t = native_recip(q);
#endif
		x_next = (f_i - c3 * x_prev) * t;
		x_prev = x_next;
		store(x_d, idx, x_prev);
		
		store(w_d, getLocalIdx(i, k+1, num_systems), - c1 * t);
	}
	
	idx += stride;

	c2 = load(b_d, idx);
	c3 = load(a_d, idx);
	f_i = load(d_d, idx);

	float4 q = (c3 * load(w_d, getLocalIdx(i, system_size-1, num_systems)) + c2);
#ifndef NATIVE_DIVIDE
	float4 t = float4(1,1,1,1) / q; 
#else
	float4 t = native_recip(q);
#endif 
	x_next = (f_i - c3 * x_prev) * t;
	x_prev = x_next;
	store(x_d, idx, x_prev);

	// backward trace
	for (int k = system_size-2; k >= 0; k--)
	{
		idx -= stride;
		x_next = load(x_d, idx);
		x_next += x_prev * load(w_d, getLocalIdx(i, k+1, num_systems));
		x_prev = x_next;
		store(x_d, idx, x_prev); 
	}
}

// This kernel is optimized to ensure all global reads and writes are coalesced,
// and to avoid bank conflicts in shared memory.  This kernel is up to 11x faster
// than the naive kernel below.  Note that the shared memory array is sized to 
// (BLOCK_DIM+1)*BLOCK_DIM.  This pads each row of the 2D block in shared memory 
// so that bank conflicts do not occur when threads address the array column-wise.
__kernel void transpose(__global float *odata, __global float *idata, int width, int height, __local float *block)
{
	int blockIdxx = get_group_id(0);
	int blockIdxy = get_group_id(1);

	int threadIdxx = get_local_id(0);
	int threadIdxy = get_local_id(1);

	// evaluate coordinates and check bounds
	int i0 = mul24(blockIdxx, BLOCK_DIM) + threadIdxx;
	int j0 = mul24(blockIdxy, BLOCK_DIM) + threadIdxy;
	
	if (i0 >= width || j0 >= height) return;

	int i1 = mul24(blockIdxy, BLOCK_DIM) + threadIdxx;
    int j1 = mul24(blockIdxx, BLOCK_DIM) + threadIdxy;
    
	if (i1 >= height || j1 >= width) return;

	int idx_a = i0 + mul24(j0, width);
    int idx_b = i1 + mul24(j1, height);

	// read the tile from global memory into shared memory
	block[threadIdxy * (BLOCK_DIM+1) + threadIdxx] = idata[idx_a];
	
	barrier(CLK_LOCAL_MEM_FENCE);
	
	// write back to transposed array
	odata[idx_b] = block[threadIdxx * (BLOCK_DIM+1) + threadIdxy];
}